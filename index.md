---
layout: default
title: "Workshop on Multilingual Multimodal Learning"
permalink: /

speakers:
  - name: David Ifeoluwa Adelani
    url: https://dadelani.github.io/
    aff: Saarland University
    topic: "Multilingual Language Model Adaptive Fine-Tuning: A Study on African Languages
    aff: Saarland University"
    interest: "Multilingual Language Model Adaptive Fine-Tuning: A Study on African Languages
    aff: Saarland University"
    image: /assets/images/Davlan.jpeg
  - name: Lisa-Anne Hendricks
    url: https://scholar.google.com/citations?user=pvyI8GkAAAAJ
    topic: "Digging Deeper into Multimodal Transformers"
    aff: DeepMind
    interest: "Digging Deeper into Multimodal Transformers"
    image: /assets/images/lisa.jpg
  - name: Lei Ji
    url: https://www.microsoft.com/en-us/research/people/leiji/
    topic: "Multimodal Video Understanding with Language Guidance"
    aff: Microsoft Research Asia
    interest: "Multimodal Video Understanding with Language Guidance"
    image: /assets/images/lei.jpg
  - name: Preethi Jyothi
    url: https://www.cse.iitb.ac.in/~pjyothi/
    topic: "New Challenges in Learning with Multilingual and Multimodal Data"
    aff: IIT Bombay
    interest: "New Challenges in Learning with Multilingual and Multimodal Data"
    image: /assets/images/preethi.png


Organizers:
  - name: Emanuele Bugliarello (University of Copenhagen)
    url: https://e-bug.github.io
    image: assets/images/test.jpeg

  - name: Kai-Wei Chang (UCLA)
    url: http://web.cs.ucla.edu/~kwchang/
    image: assets/images/test.jpeg

  - name: Desmond Elliott (University of Copenhagen)
    url: https://elliottd.github.io
    image: assets/images/test.jpeg

  - name: Spandana Gella (Amazon Alexa AI)
    url: https://scholar.google.com/citations?user=fChTW6MAAAAJ
    image: assets/images/test.jpeg

  - name: Aishwarya Kamath (NYU)
    url: https://ashkamath.github.io
    image: assets/images/test.jpeg

  - name: Liunian Harold Li (UCLA)
    url: https://liunian-harold-li.github.io
    image: assets/images/test.jpeg

  - name: Fangyu Liu (Cambridge)
    url: http://fangyuliu.me/about
    image: assets/images/test.jpeg
  
  - name: Jonas Pfeiffer (TU Darmstadt)
    url: https://pfeiffer.ai
    image: assets/images/test.jpeg

  - name: Edoardo M. Ponti (MILA Montreal)
    url: https://ducdauge.github.io
    image: assets/images/test.jpeg

  - name: Krishna Srinivasan (Google Research)
    url: https://krishna2.com/
    image: assets/images/test.jpeg

  - name: Ivan VuliÄ‡ (Cambridge)
    url: https://sites.google.com/site/ivanvulic/
    image: assets/images/test.jpeg

  - name: Yinfei Yang (Google Research)
    url: https://scholar.google.com/citations?user=kvDbu90AAAAJ
    image: assets/images/test.jpeg

  - name: Da Yin (UCLA)
    url: http://wadeyin9712.github.io
    image: assets/images/test.jpeg


---

# About

Multilingual multimodal research focuses on collecting resources, developing models, and evaluating systems that need to jointly reason over multilingual text and multimodal inputs, including images, videos, texts, and knowledge bases. Multilingual multimodal NLP presents new and unique challenges. First, it is one of the areas that suffer the most from language imbalance issues. Texts in most multimodal datasets are usually only available in high-resource languages. Second, multilingual multimodal research provides opportunities to investigate culture-related phenomena. On top of the language imbalance issue in text-based corpora and models, the data of additional modalities (e.g. images or videos) are mostly collected from North American and Western European sources (and their worldviews). As a result, multimodal models do not capture our world's multicultural diversity and do not generalise to out-of-distribution data from minority cultures. The interplay of the two issues leads to extremely poor performance of multilingual multimodal systems in real-life scenarios. This workshop encourages and promotes research efforts towards more inclusive multimodal technologies and tools to assess them. We invite papers which focus on the topics of interest include (but are not limited to):

- Datasets for multilingual multimodal learning
- Modeling multilingual multimodal Data
- Approaches to improving the inclusion of multilingual multimodal models
- Evaluation and analysis for multilingual multimodal learning
- Future challenges of multilingual multimodal research


# Invited Talks (In alphabetical order)

{% include team.html id="speakers" %}


# Important Dates 

Important Dates:
- February 28, 2022: Submission Date
- March 26, 2022: Notification of Acceptance
- April 10, 2022: Camera-ready papers due (hard deadline)
- May 27, 2022: Workshop on Multimodal Multilingual Learning


# Organizers and Contact

Organizers are in the alphabetical order. For any question, please contact [mml DOT wksp AT gmail DOT com](mailto:mml.wksp@gmail.com).

<ul>
{% for p in page.Organizers %}
<li>
<a{% if p.url %} href="{{ p.url }}"{% endif %}>{{ p.name }}</a>
</li>
{% endfor %}
</ul>

#### Follow Us

Twitter: [@MML_WKSP](https://twitter.com/MML_WKSP)

# Sponsors

We are grateful for the generous funding from our list of sponsors:

 <img src="/assets/images/640px-Apple-logo.png" height="50" width="50"> <img src="/assets/images/google.png" height="55" width="128"> 
