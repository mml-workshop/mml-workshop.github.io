<!DOCTYPE html>
<html lang="en-US">
  <head>

    <link rel="shortcut icon" href="/assets/images/logo.png">

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Workshop on Multilingual Multimodal Learning | Co-located with ACL 2022</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Workshop on Multilingual Multimodal Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Co-located with ACL 2022" />
<meta property="og:description" content="Co-located with ACL 2022" />
<link rel="canonical" href="https://mml-workshop.github.io/shared_task" />
<meta property="og:url" content="https://mml-workshop.github.io/shared_task" />
<meta property="og:site_name" content="Workshop on Multilingual Multimodal Learning" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Workshop on Multilingual Multimodal Learning" />
<script type="application/ld+json">
{"description":"Co-located with ACL 2022","headline":"Workshop on Multilingual Multimodal Learning","@type":"WebPage","url":"https://mml-workshop.github.io/shared_task","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
  </head>
  <body>
    <header class="page-header" role="banner">
      <h1 class="project-name">Workshop on Multilingual Multimodal Learning</h1>
      <h2 class="project-tagline">Co-located with ACL 2022</h2>
      <a href="index.html" class="btn">About</a>
      <a href="submit.html" class="btn">Submit</a>
      <a href="schedule.html" class="btn">Schedule</a>
      <a href="accepted.html" class="btn">Accepted Papers</a>
      <a href="shared_task.html" class="btn">Shared Task</a>
    </header>

    <main id="content" class="main-content" role="main">
      <h2 id="mml-2022-shared-task">MML 2022 Shared Task</h2>

<p>The multilingual multimodal learning (MML) workshop, co-located at ACL 2022, is hosting a shared task on multilingual visually grounded reasoning. The task will be centred around the MaRVL dataset, introduced by <a href="https://marvl-challenge.github.io">Liu et al. (EMNLP 2021)</a>. This dataset extends the NLVR2 task <a href="https://lil.nlp.cornell.edu/nlvr/">(Suhr et al., ACL 2019)</a> to multicultural and multilingual (Indonesian, Mandarin, Swahili, Tamil, Turkish) inputs: Given two images and a textual description, a system needs to predict whether the description applies to both images (True/False).</p>

<p>The standard setup consists of fine-tuning a multilingual vision-and-language model in the English NLVR2 dataset and then evaluating on MaRVL. We consider two subtasks, as detailed below: zero-shot transfer and few-shot transfer. Both setups have been shown to be challenging <a href="https://iglue-benchmark.github.io">(Bugliarello et al., 2022)</a>, and we look forward to seeing your approaches to the tasks!</p>

<p>Participants will be invited to describe their system in a paper for the MML workshop proceedings. The task organisers will write an overview paper that describes the task and summarises the different approaches taken, and analyses their results.</p>

<h3 id="important-links">Important Links</h3>
<p>Here are the links to quickly download the data for the shared task:</p>
<ul>
  <li>NLVR2: <a href="https://github.com/e-bug/iglue/tree/main/datasets/nlvr2/annotations">text</a> / <a href="https://github.com/lil-lab/nlvr/tree/master/nlvr2#direct-image-download">images</a> / <a href="https://sid.erda.dk/sharelink/FjJUsFbRWO">features</a></li>
  <li>MaRVL zero-shot: <a href="https://github.com/e-bug/iglue/tree/main/datasets/marvl/zero_shot">text</a> / <a href="https://dataverse.scholarsportal.info/dataset.xhtml?persistentId=doi:10.5683/SP3/42VZ4P">images</a> / <a href="https://sid.erda.dk/sharelink/fMNmRmJgQA">features</a></li>
  <li>MaRVL most-shot: <a href="https://github.com/e-bug/iglue/tree/main/datasets/marvl/few_shot/annotations">text</a> / <a href="https://dataverse.scholarsportal.info/dataset.xhtml?persistentId=doi:10.5683/SP3/42VZ4P">images</a> / <a href="https://sid.erda.dk/sharelink/fMNmRmJgQA">features</a></li>
  <li><a href="https://github.com/e-bug/iglue">IGLUE repository</a></li>
</ul>

<p>Preprocessed image features are available for two visual encoders:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">*boxes36</code>: 36 RoIs per image extracted with a Faster R-CNN with a ResNet-101</li>
  <li><code class="language-plaintext highlighter-rouge">*X101</code>: 10-100 RoIs per image extracted with a Faster R-CNN with a ResNeXt-101</li>
</ul>

<p>The IGLUE repository contains sample code and pretrained models to help you get started. Open issues on GitHub or reach out to us for any doubts.</p>

<h3 id="important-dates">Important Dates</h3>
<ul>
  <li>Submission Due: April 30 2022 (11:59pm AoE)</li>
  <li>Notification: May 7 2022 (11:59pm AoE)</li>
  <li>Camera-ready Due: May 14 2022 (11:59pm AoE)</li>
  <li>Workshop: 27 May 2022</li>
</ul>

<h3 id="subtasks">Subtasks</h3>
<p>The shared task will consist of two subtasks:</p>
<ul>
  <li>ZS) Zero-shot transfer: Models are fine-tuned on the English NLVR2 data, and tested on MaRVL Indonesian, Mandarin, Swahili, Tamil, Turkish</li>
  <li>MS) Most-shot transfer: Models are further fine-tuned on a few data points in the target language. This subtask corresponds to the most-shot setup of Bugliarello et al. (2022). In particular, performance is only reported in three languages: Indonesian, Mandarin and Turkish.</li>
</ul>

<p>NB: we will <em>only</em> consider submissions that use pre-existing pre-trained models that are publicly available or new models that have been (pre)trained on publicly available data.</p>

<p>“Translate test” methods are accepted but will be ranked separately.</p>

<h3 id="submission">Submission</h3>
<p>Submissions should be emailed to the organisers by the end of April 30, anywhere on Earth.
Submissions need to follow the jsonlines format, where languages are in ISO 639-2 codes:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{"concept": "39-Panci", "language": "id", "chapter": "Basic actions and technology", "id": "id-0", "prediction": true}
</code></pre></div></div>
<p>Files should be named as <code class="language-plaintext highlighter-rouge">{team-name}_{zs/ms}_{xl/tt}_{lang}.jsonl</code> to indicate the subtask (zero-shot or most-shot), whether it’s cross-lingual or translate-test transfer, and the target language.</p>

<h3 id="description-papers">Description Papers</h3>
<p>Papers describing shared task submissions should consist of 4 to 8 pages of content plus additional pages of references, formatted according to the ARR format guidelines for ACL 2022. For shared task paper submission, it is not necessary to blind the team name and authors. Accepted papers will be published online in the ACL 2022 proceedings and will be presented at the MML workshop at ACL 2022. Writeups should be submitted through <a href="https://openreview.net/group/edit?id=aclweb.org/ACL/2022/Workshop/MML">OpenReview</a>, and are due by 30 April 2022 11:59pm [UTC-12h].</p>


    </main>
  </body>
</html>
